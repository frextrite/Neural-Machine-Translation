{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "## based on the paper Sequence to Sequence Learning with Neural Networks\n",
    "##### Dataset used: IWSLT'15 English-Vietnamese data [Small]\n",
    "##### Link: https://nlp.stanford.edu/projects/nmt/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Machine Translation\n",
    "\n",
    "\n",
    "Machine Translation using Neural Networks is implemented using sequence to sequence learning. In seq2seq learning, an input, which, as the name suggests, is a sequence, is mapped to the output, which is also a sequence. Thus se2seq learning can be used in various areas like Machine Translation, Chatbots, QnA solver.\n",
    "\n",
    "Deep Neural Networks(DNNs) are powerful model that work well whenever large labeled training data sets are available, however they cannot be used for mapping sequences. However DNNs can only be used with fixed dimensionality input and output vectors. It is a significant limitation since many areas require sequential inputs with varied lengths, for example, speech recognition and machine translation. To overcome this, Neural Machine Translation uses multi-layer Long Short-Term Memory(LSTM) to map input sequence to a vector of fixed dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset details\n",
    "\n",
    "The original model used WMT'14 English to French dataset. The model was trained on a subset of 12M sentences which consisted of 348M French words and 304M English words. \n",
    "\n",
    "Vocabulary of 160000 most frequent English words and 80000 most frequent French words was used. Every out of vocabulary word was replaced with a special unknown token.\n",
    "\n",
    "However, we'll use the English to Vietnamese dataset. This dataset is small in size and great for learning about seq2seq model and architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the data for training\n",
    "1. Download the dataset files from https://nlp.stanford.edu/projects/nmt/ (train.en, train.vi)\n",
    "2. Create a folder named dataset in the current directory<br>\n",
    "```mkdir dataset```\n",
    "3. Move files into the dataset folder<br>\n",
    "```mv train.en train.vi ./dataset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset location holders\n",
    "train_en = './dataset/train.en'\n",
    "train_vi = './dataset/train.vi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the translation files\n",
    "datafile_en = open(train_en, 'r')\n",
    "datafile_vi = open(train_vi, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of all input and output sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the training sentences from the files\n",
    "sentences_en = datafile_en.readlines()\n",
    "sentences_vi = datafile_vi.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133317\n",
      "133317\n",
      "['Rachel Pike : The science behind a climate headline\\n', 'In 4 minutes , atmospheric chemist Rachel Pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule .\\n']\n",
      "['Khoa học đằng sau một tiêu đề về khí hậu\\n', 'Trong 4 phút , chuyên gia hoá học khí quyển Rachel Pike giới thiệu sơ lược về những nỗ lực khoa học miệt mài đằng sau những tiêu đề táo bạo về biến đổi khí hậu , cùng với đoàn nghiên cứu của mình -- hàng ngàn người đã cống hiến cho dự án này -- một chuyến bay mạo hiểm qua rừng già để tìm kiếm thông tin về một phân tử then chốt .\\n']\n"
     ]
    }
   ],
   "source": [
    "# play with data\n",
    "print(len(sentences_en))\n",
    "print(len(sentences_vi))\n",
    "print(sentences_en[:2])\n",
    "print(sentences_vi[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total number of training samples\n",
    "num_samples = len(sentences_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the training data\n",
    "1. Trailing ```\\n``` is removed from each sentence\n",
    "2. Each sentence is converted to lower case\n",
    "3. A special start and end token is added to the sentence to mark the start and end of the sentence. Our model will learn this behaviour over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define start, end and an unknown token\n",
    "start = 'startseq'\n",
    "end = 'endseq'\n",
    "unknown = 'unk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess english sentences\n",
    "\n",
    "sentences_en_processed = []\n",
    "for sentence in sentences_en:\n",
    "    # convert sentence to lower case\n",
    "    sentence = sentence.lower()\n",
    "    # remove trailing \\n from the sentence\n",
    "    sentence = sentence.rstrip()\n",
    "    # add start and end tokens\n",
    "    sentence = start + ' ' + sentence + ' ' + end\n",
    "    \n",
    "    # add the sentence to the processed sentenceslist\n",
    "    sentences_en_processed.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess vietnamemes sentences\n",
    "\n",
    "sentences_vi_processed = []\n",
    "for sentence in sentences_vi:\n",
    "    # convert sentence to lower case\n",
    "    sentence = sentence.lower()\n",
    "    # remove trailing \\n from the sentence\n",
    "    sentence = sentence.rstrip()\n",
    "    # add start and end tokens\n",
    "    sentence = start + ' ' + sentence + ' ' + end\n",
    "    \n",
    "    # add the sentence to the processed sentenceslist\n",
    "    sentences_vi_processed.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['startseq recently the headlines looked like this when the intergovernmental panel on climate change , or ipcc , put out their report on the state of understanding of the atmospheric system . endseq', 'startseq that report was written by 620 scientists from 40 countries . endseq']\n",
      "['startseq các tiêu đề gần đây trông như thế này khi ban điều hành biến đổi khí hậu liên chính phủ , gọi tắt là ipcc đưa ra bài nghiên cứu của họ về hệ thống khí quyển . endseq', 'startseq nghiên cứu được viết bởi 620 nhà khoa học từ 40 quốc gia khác nhau . endseq']\n"
     ]
    }
   ],
   "source": [
    "print(sentences_en_processed[5:7])\n",
    "print(sentences_vi_processed[5:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a character array of all the vietnamese characters\n",
    "char_array_vi = []\n",
    "for sentence in sentences_vi:\n",
    "    sentence = sentence.lower()\n",
    "    for char in sentence:\n",
    "        char_array_vi.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a character array of all the english characters\n",
    "char_array_en = []\n",
    "for sentence in sentences_en:\n",
    "    sentence = sentence.lower()\n",
    "    for char in sentence:\n",
    "        char_array_en.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dictionary of words and their frequency for english characters\n",
    "from collections import Counter\n",
    "char_dict_en = dict(Counter(char_array_en))\n",
    "len(char_dict_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dictionary of words and their frequency for vietnamese characters\n",
    "from collections import Counter\n",
    "char_dict_vi = dict(Counter(char_array_vi))\n",
    "len(char_dict_vi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "## based on the paper Sequence to Sequence Learning with Neural Networks\n",
    "##### Dataset used: IWSLT'15 English-Vietnamese data [Small]\n",
    "##### Link: https://nlp.stanford.edu/projects/nmt/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Machine Translation\n",
    "\n",
    "\n",
    "Machine Translation using Neural Networks is implemented using sequence to sequence learning. In seq2seq learning, an input, which, as the name suggests, is a sequence, is mapped to the output, which is also a sequence. Thus se2seq learning can be used in various areas like Machine Translation, Chatbots, QnA solver.\n",
    "\n",
    "Deep Neural Networks(DNNs) are powerful model that work well whenever large labeled training data sets are available, however they cannot be used for mapping sequences. However DNNs can only be used with fixed dimensionality input and output vectors. It is a significant limitation since many areas require sequential inputs with varied lengths, for example, speech recognition and machine translation. To overcome this, Neural Machine Translation uses multi-layer Long Short-Term Memory(LSTM) to map input sequence to a vector of fixed dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset details\n",
    "\n",
    "The original model used WMT'14 English to French dataset. The model was trained on a subset of 12M sentences which consisted of 348M French words and 304M English words. \n",
    "\n",
    "Vocabulary of 160000 most frequent English words and 80000 most frequent French words was used. Every out of vocabulary word was replaced with a special unknown token.\n",
    "\n",
    "However, we'll use the English to French, European Parliament Proceedings Parallel Corpus dataset. This dataset is great for learning about seq2seq model and architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the dataset\n",
    "1. Download the dataset files from http://www.statmt.org/europarl/index.html (parallel corpus French-English)\n",
    "2. Create a folder dataset in the current directory<br>\n",
    "```mkdir dataset```\n",
    "3. Extract and move files into the dataset folder<br>\n",
    "```tar xvzf fr-en.tgz ./dataset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset location holders\n",
    "fileloc_en = './dataset/europarl-v7.fr-en.en'\n",
    "fileloc_fr = './dataset/europarl-v7.fr-en.fr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open data files\n",
    "file_en = open(fileloc_en, 'r')\n",
    "file_fr = open(fileloc_fr, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dictionary and Preprocess data\n",
    "Dictionary(also known as vocabulary) is a list of most frequent words in a particular language. Here, we are working with English to French translation, hence, we will create 2 dictionaries, one for English and the other for French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define language tokens\n",
    "start_token = 'startseq'\n",
    "end_token = 'endseq'\n",
    "unknown_token = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this module provides regular expression matching operations similar to those found in Perl\n",
    "import re\n",
    "\n",
    "def preprocess_sentence(w, add_tokens=False):\n",
    "    # convert each character to lower case\n",
    "    w = w.lower()\n",
    "    \n",
    "    # create a space between word and the punctuation following it\n",
    "    w = re.sub(r\"([?.!,?])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    # replace everything with space except (a-Z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    # w = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    # add the start and end token to the sentence\n",
    "    # the model will learn this behaviour over time\n",
    "    if add_tokens:\n",
    "        w = start_token + ' ' + w + ' ' + end_token\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary\n",
    "We'll create our own vocabulary from the words present in the English and French corpus.The vocabulary will consist of 50000 most frequent English words and 50000 most frequent French words.\n",
    "\n",
    "#### Steps:\n",
    "1. Read all the words from the files\n",
    "2. Create a set consisting of all the unique words\n",
    "3. Extract the most frequent words and create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports for creating a dictionary\n",
    "from collections import Counter\n",
    "from pickle import load, dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab file locations\n",
    "vocab_loc_en = './dataset/vocab.en'\n",
    "vocab_loc_fr = './dataset/vocab.fr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Dictionary class for much better code representation\n",
    "class Dictionary:\n",
    "    def __init__(self, input_file):\n",
    "        self.vocab = set()\n",
    "        self.word_list = []\n",
    "        self.processed_sentences = []\n",
    "        self.input_file = input_file\n",
    "        \n",
    "    # sets up the dictionary object\n",
    "    def set_up(self):\n",
    "        self.read_file()\n",
    "        self.preprocess()\n",
    "        self.create_word_list()\n",
    "        self.file.close()\n",
    "        self.processed_sentences = []\n",
    "        \n",
    "    # frees up a bit of space by deallocating the list\n",
    "    def tear_down(self):\n",
    "        self.word_list = []\n",
    "    \n",
    "    # read the data file\n",
    "    def read_file(self):\n",
    "        self.file = open(self.input_file, 'r')\n",
    "        self.sentences = self.file.readlines()\n",
    "    \n",
    "    # pro-processes the data, check above for further details\n",
    "    def preprocess(self):\n",
    "        for w in self.sentences:\n",
    "            self.processed_sentences.append(preprocess_sentence(w))\n",
    "    \n",
    "    # creates a full list of words\n",
    "    def create_word_list(self):\n",
    "        for sentence in self.processed_sentences:\n",
    "            for word in sentence.split(' '):\n",
    "                self.word_list.append(word)\n",
    "    \n",
    "    # creates a list of num_frequent most frequent word in the dataset\n",
    "    def create_vocab(self, num_frequent=49997):\n",
    "        self.vocab = list(dict(Counter(self.word_list).most_common(num_frequent)).keys())\n",
    "    \n",
    "    # returns the size of vocabulary\n",
    "    def get_total_words(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    # returns the vocabulary\n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "    \n",
    "    # pickle the vocab to a file so that we do not have to create it again and again\n",
    "    def save_vocab(self, vocab_file):\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            dump(self.vocab, f)\n",
    "    \n",
    "    # load the vocab from the pickled file\n",
    "    def load_vocab(self, vocab_file):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.vocab = load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an english dictionary object\n",
    "dict_english = Dictionary(fileloc_en)\n",
    "dict_english.set_up()\n",
    "dict_english.create_vocab()\n",
    "dict_english.tear_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify if everything is working fine\n",
    "vocab_en = dict_english.get_vocab()\n",
    "vocab_en[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an english dictionary object\n",
    "dict_french = Dictionary(fileloc_fr)\n",
    "dict_french.set_up()\n",
    "dict_french.create_vocab()\n",
    "dict_french.tear_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify if everything is working fine\n",
    "vocab_fr = dict_french.get_vocab()\n",
    "vocab_fr[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append special tokens to the vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [unknown_token, end_token, start_token]\n",
    "\n",
    "extend_vocab_en = lambda x: vocab_en.insert(0, x)\n",
    "extend_vocab_fr = lambda x: vocab_fr.insert(0, x)\n",
    "[extend_vocab_en(x) for x in list]\n",
    "[extend_vocab_fr(x) for x in list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify if the vocabs are just fine\n",
    "vocab_en[:10], vocab_fr[:10], len(vocab_en), len(vocab_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the vocab to file so that we do not have to create the vocab again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the vocab in a file\n",
    "dict_english.save_vocab(vocab_loc_en)\n",
    "dict_french.save_vocab(vocab_loc_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the vocab if we've already pickled it before(saves a lot of time and memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Dictionary objects and load the vocab into the object\n",
    "dict_english = Dictionary(fileloc_en)\n",
    "dict_english.load_vocab(vocab_loc_en)\n",
    "\n",
    "dict_french = Dictionary(fileloc_fr)\n",
    "dict_french.load_vocab(vocab_loc_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vocab into variables\n",
    "vocab_en = dict_english.get_vocab()\n",
    "vocab_fr = dict_french.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['startseq', 'endseq', '<unk>', 'the', ',', '.', 'of', 'to', 'and', 'in'],\n",
       " ['startseq', 'endseq', '<unk>', 'de', ',', '.', 'la', 'et', 'le', 'les'],\n",
       " 50000,\n",
       " 50000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify if the vocabs are just fine\n",
    "vocab_en[:10], vocab_fr[:10], len(vocab_en), len(vocab_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data\n",
    "Raw real world data may be incomplete, noisy, or inconsistent which may affect the training accuracy of our model. Hence to overcome this, the raw data is generally cleaned before feeding it into the model. This cleaning process is known as Data Preprocessing.\n",
    "\n",
    "Preprocessing language data may include some additional steps which we will see below.\n",
    "\n",
    "#### Steps:\n",
    "1. Convert sentences to lower case\n",
    "2. Clean sentences by removing trailing ```\\n``` and other punctuations\n",
    "3. Add a _start_ and _end_ token to each sentence which helps the model to determine the start and end of a sentence(Our model will learn this behaviour over time)\n",
    "4. Replace words which are not in the vocabulary by a special unknown token\n",
    "\n",
    "Example: The sentence ```That person is sitting on the bench, and enjoying the cool breeze.``` is converted to ```startseq that person is sitting on the beach and enjoying the cool breeze endseq```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of sentences we would be training our model for learning purposes\n",
    "training_samples = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all the sentences\n",
    "sentences_en = file_en.readlines()\n",
    "sentences_fr = file_fr.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the translation files\n",
    "datafile_en = open(fileloc_en, 'r')\n",
    "datafile_fr = open(fileloc_fr, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the training sentences from the files\n",
    "sentences_en = datafile_en.readlines()\n",
    "sentences_fr = datafile_fr.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007723\n",
      "2007723\n",
      "['Resumption of the session\\n', 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\n']\n",
      "['Reprise de la session\\n', 'Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.\\n']\n"
     ]
    }
   ],
   "source": [
    "# play with data\n",
    "print(len(sentences_en))\n",
    "print(len(sentences_fr))\n",
    "print(sentences_en[:2])\n",
    "print(sentences_fr[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total number of training samples\n",
    "num_samples_en = len(sentences_en)\n",
    "num_samples_fr = len(sentences_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess english sentences\n",
    "\n",
    "sentences_en_processed = []\n",
    "for sentence in sentences_en[:training_samples]:\n",
    "    sentence = preprocess_sentence(sentence, add_tokens=True)\n",
    "    sentence_processed = \"\"\n",
    "    for w in sentence.split(' '):\n",
    "        if w not in vocab_en:\n",
    "            sentence_processed += \" \" + unknown_token\n",
    "        else:\n",
    "            sentence_processed += \" \" + w\n",
    "    sentences_en_processed.append(sentence_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' startseq resumption of the session endseq',\n",
       " ' startseq i declare resumed the session of the european parliament adjourned on friday 17 december 1999 , and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period . endseq',\n",
       " \" startseq although , as you will have seen , the dreaded 'millennium <unk> failed to materialise , still the people in a number of countries suffered a series of natural disasters that truly were dreadful . endseq\",\n",
       " ' startseq you have requested a debate on this subject in the course of the next few days , during this part-session . endseq',\n",
       " \" startseq in the meantime , i should like to observe a minute' s silence , as a number of members have requested , on behalf of all the victims concerned , particularly those of the terrible storms , in the various countries of the european union . endseq\",\n",
       " \" startseq please rise , then , for this minute' s silence . endseq\",\n",
       " \" startseq (the house rose and observed a minute' s silence) endseq\",\n",
       " ' startseq madam president , on a point of order . endseq',\n",
       " ' startseq you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka . endseq',\n",
       " ' startseq one of the people assassinated very recently in sri lanka was mr <unk> <unk> , who had visited the european parliament just a few months ago . endseq']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_en_processed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess french sentences\n",
    "\n",
    "sentences_fr_processed = []\n",
    "for sentence in sentences_fr[:training_samples]:\n",
    "    sentence = preprocess_sentence(sentence, add_tokens=True)\n",
    "    sentence_processed = \"\"\n",
    "    for w in sentence.split(' '):\n",
    "        if w not in vocab_fr:\n",
    "            sentence_processed += \" \" + unknown_token\n",
    "        else:\n",
    "            sentence_processed += \" \" + w\n",
    "    sentences_fr_processed.append(sentence_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' startseq resumption of the session endseq', ' startseq i declare resumed the session of the european parliament adjourned on friday 17 december 1999 , and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period . endseq', \" startseq although , as you will have seen , the dreaded 'millennium <unk> failed to materialise , still the people in a number of countries suffered a series of natural disasters that truly were dreadful . endseq\", ' startseq you have requested a debate on this subject in the course of the next few days , during this part-session . endseq', \" startseq in the meantime , i should like to observe a minute' s silence , as a number of members have requested , on behalf of all the victims concerned , particularly those of the terrible storms , in the various countries of the european union . endseq\", \" startseq please rise , then , for this minute' s silence . endseq\", \" startseq (the house rose and observed a minute' s silence) endseq\", ' startseq madam president , on a point of order . endseq', ' startseq you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka . endseq', ' startseq one of the people assassinated very recently in sri lanka was mr <unk> <unk> , who had visited the european parliament just a few months ago . endseq']\n",
      "[' startseq reprise de la session endseq', ' startseq je déclare reprise la session du parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances . endseq', \" startseq comme vous avez pu le constater , le grand bogue de l'an 2000 ne s'est pas produit . en revanche , les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles . endseq\", ' startseq vous avez souhaité un débat à ce sujet dans les prochains jours , au cours de cette période de session . endseq', \" startseq en attendant , je souhaiterais , comme un certain nombre de collègues me l'ont demandé , que nous observions une minute de silence pour toutes les victimes , des tempêtes notamment , dans les différents pays de l'union européenne qui ont été touchés . endseq\", ' startseq je vous invite à vous lever pour cette minute de silence . endseq', ' startseq (le parlement , debout , observe une minute de silence) endseq', \" startseq madame la présidente , c'est une motion de procédure . endseq\", ' startseq vous avez probablement appris par la presse et par la télévision que plusieurs attentats à la bombe et crimes ont été perpétrés au sri lanka . endseq', \" startseq l'une des personnes qui vient d'être assassinée au sri lanka est m . <unk> <unk> , qui avait rendu visite au parlement européen il y a quelques mois à peine . endseq\"]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_en_processed[:10])\n",
    "print(sentences_fr_processed[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
